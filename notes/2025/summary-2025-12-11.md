# Meeting Summary for AI WG

**NOTICE**: This summary was auto-generated by Zoom's "AI". AI-generated
content may be inaccurate or misleading. Always check for accuracy. If in
doubt, please consult the meeting recording at
https://youtube.com/@GraphQLFoundation/playlists

- Meeting start: 2025-12-11T18:28:06Z
- Meeting end: 2025-12-11T19:33:42Z
- Summary start: 2025-12-11T18:30:12Z
- Summary end: 2025-12-11T19:31:01Z

The GraphQL AI Working Group meeting began with introductions and discussions about recent events, including the GraphQL Day conference and participation guidelines. The group explored new approaches for efficient GraphQL query generation using embeddings and subschemas, with presentations on methods for retrieving relevant types and queries. The conversation ended with benchmarking results for different language models and discussions about potential next steps for improving query generation and performance.

## Next Steps

- Alex: Reach out to Thore to collaborate on creating an open source version of the query graders and integrate them into Thore's benchmark.
- Alex: Publish updated benchmark results comparing LLMs (including Claude, GPT-5, etc.) on GraphQL query generation, including the new graders.
- Dale: Share the blog post about Apollo MCP server's token usage reduction/minification approach in the chat.
- Kewei: Internally try out Thore's field-level embedding approach at Meta and potentially share results in a future meeting.
- Kewei: Connect with Michael to explore publishing snippets from this meeting to YouTube, with participants' permission.
- Raymie: Add meeting notes to the Google Doc linked in the agenda markdown file.
- (Optional/Implied) Hugh/Kewei: Follow up in a couple of months to share comparative results between the subschema and introspection search approaches for schema context management.
- (Optional/Implied) Alex/Thore: Integrate Thore's semantic search into the benchmark to measure improvement over baseline LLM performance (if explicitly agreed; otherwise, remove if not directly stated as a commitment).

## Summary

### GraphQL AI Working Group Meeting

The GraphQL AI Working Group meeting began with introductions and a reminder about participation guidelines and code of conduct. Kewei, the host from Meta, explained that meetings would be livestreamed and recorded for YouTube, with potential use of LLM and AI for summarization. The group discussed the recent GraphQL Day conference, noting it was smaller than expected but had good talks, though recordings would not be distributed effectively. The conversation ended with a request for attendees to share their locations and organizations, though Roy was absent.

### GraphQL Grants and Schema Search

The meeting began with introductions from participants, including Thor, Mark, Michael, Hugh, Dale, Raymie, Alex, Jeff, and Pascal, who discussed their affiliations and locations. Kewei outlined the agenda and reminded everyone about GraphQL grants for key initiatives, encouraging applications to the TSC committee. The first agenda item was similarity search to navigate schema, with Thor set to lead the discussion.

### Vector Embeddings for Semantic Search

The team discussed a method for retrieving relevant types and queries using vector embeddings from OpenAI. Thore presented a flattened approach where type-field combinations are encoded as vectors for search, though questions were raised about potential performance issues due to increased database size. Pascal suggested an alternative approach of starting from the user query and recursively searching for relevant fields, potentially using multiple paths to find the desired information. Michael explained that in federation, they use lookups to efficiently resolve data across microservices, which is different from the current approach of using vector embeddings for semantic search.

### Efficient GraphQL Query Generation

The meeting focused on two main topics: a new approach for efficient GraphQL query generation using embeddings and subschemas, and benchmarking the performance of different language models for generating GraphQL queries. Hugh presented a method to dynamically load subschemas to manage the context window size for LLMs, which addresses the scalability issue of large enterprise GraphQL schemas. Pascal raised concerns about the size of subschemas and suggested exploring a combination approach with semantic introspection. Alex shared results from benchmarking different LLMs, showing that Claude performs significantly better than GPT-4.1 and GPT-5 in generating valid GraphQL queries. The group discussed potential next steps, including integrating the grader into Thor's benchmark and exploring the use of RAG with state-of-the-art GPT models.
